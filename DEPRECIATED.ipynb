{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Here Has Been Transferred to a Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/target/Empire-State-Building.jpg'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_file = 'Empire-State-Building.jpg'\n",
    "style_file = 'Natasha-Russu-Eye.jpg'\n",
    "\n",
    "def get_img_paths(target, style):\n",
    "    target = 'images/target/' + target\n",
    "    style = 'images/style/' + style\n",
    "    return target, style\n",
    "\n",
    "target_image_path, style_reference_image_path = get_img_paths(target_file, style_file)\n",
    "target_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_width_height(target_image_path): \n",
    "    \"\"\"\n",
    "    Get dimensions of the generated picture.\n",
    "    \n",
    "    :param target_image_path: Path of the target (reference) image\n",
    "    :type  target_image_path: str\n",
    "    :returns: height, width\n",
    "    :rtype:   (int, int)\n",
    "    \"\"\"\n",
    "    \n",
    "    width, height = load_img(target_image_path).size\n",
    "    img_height = 400\n",
    "    img_width = int(width * img_height / height)\n",
    "    return img_height, img_width\n",
    "\n",
    "img_height, img_width = get_width_height(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Open, resize, and format pictures into tensors.\n",
    "    \n",
    "    :param image_path: path of the image\n",
    "    :type  image_path: str\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    \"\"\"\n",
    "    \n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    ''' \n",
    "    This reverses a transformation done by vgg19.preprocess_input.\n",
    "    Basically converts tensors back to images.\n",
    "    '''\n",
    "    \n",
    "    # Remove mean pixel to zero center\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    # Convert images from BGR to RGB\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(target_path, style_path, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Creates the model to train on.\n",
    "\n",
    "    :param target_path: The path of the target image. \n",
    "    :type  target_path: str\n",
    "    :param style_path: The path of the style image\n",
    "    :type  style_path: str\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    :returns: (Pretrained VGG19 model, Tensorflow placeholder for generated image)\n",
    "    :rtype:   (keras.engine.training.Model, tensorflow.python.framework.ops.Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    # note that these two images do not change\n",
    "    target_image = K.constant(\n",
    "        preprocess_image(target_path, img_height, img_width)\n",
    "    )\n",
    "    style_reference_image = K.constant(\n",
    "        preprocess_image(style_path, img_height, img_width)\n",
    "    )\n",
    "\n",
    "    # placeholder for generated image\n",
    "    combination_image = K.placeholder(\n",
    "        (1, img_height, img_width, 3)\n",
    "    )\n",
    "\n",
    "    # combine into single branch\n",
    "    input_tensor = K.concatenate(\n",
    "        [target_image,\n",
    "         style_reference_image,\n",
    "         combination_image],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    # build VGG19 network with the three images as input\n",
    "    model = vgg19.VGG19(\n",
    "        input_tensor=input_tensor,\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "    return model, combination_image\n",
    "\n",
    "\n",
    "model, combination_image = create_model(\n",
    "    target_image_path,\n",
    "    style_reference_image_path, img_height, img_width)\n",
    "# model.summary()\n",
    "# combination_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    \"\"\"\n",
    "    Compute the content loss.\n",
    "\n",
    "    :param base: The tensor representing a layer of the base \n",
    "    :type  base: tensorflow.python.framework.ops.Tensor\n",
    "    :param combination: The tensor representing layer of the combination of the target and the style\n",
    "    :type  combination: tensorflow.python.framework.ops.Tensor\n",
    "    :returns: Scaler of the content loss\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"\n",
    "    Computes gram matrix of an input matrix.\n",
    "\n",
    "    :param x: The tensor representing the layer\n",
    "    :type  x: tensorflow.python.framework.ops.Tensor\n",
    "    :returns: the inner product of the feature maps of a layer\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def style_loss(style, combination, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Compute the style loss for one layer\n",
    "\n",
    "    :param style: The tensor representing gram matrix of the style\n",
    "    :type  style: tensorflow.python.framework.ops.Tensor\n",
    "    :param combination: The tensor representing gram matrix of the combination\n",
    "    :type  combination: tensorflow.python.framework.ops.Tensor\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    :returns: Scaler of the style loss\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "def total_variation_loss(x, img_height, img_width):\n",
    "    \"\"\"\n",
    "    This operates on the pixels of the generated image.\n",
    "    Sort of like a regularization loss to make sure the image isn't overly pixelated.\n",
    "\n",
    "    :param x: The tensor representing the generated image\n",
    "    :type  x: tensorflow.python.framework.ops.Tensor\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    :returns: Scaler of the style loss\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_loss(\n",
    "        model,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        combination_image,\n",
    "        total_variation_weight=1e-4,\n",
    "        style_weight=1.,\n",
    "        content_weight=0.025):\n",
    "    \"\"\"\n",
    "    Calculate total loss by a weighted average of the three above.\n",
    "\n",
    "    :param model: Pretrained VGG19 model\n",
    "    :type  model: keras.engine.training.Model\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    :param combination_image: Tensorflow placeholder for generated image\n",
    "    :type  combination_image: tensorflow.python.framework.ops.Tensor\n",
    "    :param total_variation_weight: Weight of the total variation loss\n",
    "    :type  total_variation_weight: float\n",
    "    :param style_weight: Weight of the style loss\n",
    "    :type  style_weight: float\n",
    "    :param content_weight: Weight of the content loss\n",
    "    :type  content_weight: float\n",
    "    :returns: placeholder for the total loss\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # maps layer names to activation tensors\n",
    "    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "    content_layer = 'block5_conv2'\n",
    "    style_layers = ['block1_conv1',\n",
    "                    'block2_conv1',\n",
    "                    'block3_conv1',\n",
    "                    'block4_conv1',\n",
    "                    'block5_conv1']\n",
    "\n",
    "    # add content loss\n",
    "    loss = K.variable(0.)\n",
    "    layer_features = outputs_dict[content_layer]\n",
    "    target_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss += content_weight * content_loss(\n",
    "        target_image_features,\n",
    "        combination_features\n",
    "    )\n",
    "\n",
    "    # adds style loss component for each layer\n",
    "    for layer_name in style_layers:\n",
    "        layer_features = outputs_dict[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(\n",
    "            style_reference_features,\n",
    "            combination_features,\n",
    "            img_height,\n",
    "            img_width\n",
    "        )\n",
    "        loss += (style_weight / len(style_layers)) * sl\n",
    "\n",
    "    # add variational loss\n",
    "    loss += total_variation_weight * total_variation_loss(\n",
    "        combination_image,\n",
    "        img_height,\n",
    "        img_width\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_image_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "combination_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "content_loss <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "style_reference_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "combination_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "style_loss <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "combination_image <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "total_variation_loss <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = find_loss(model, img_height,\n",
    "                 img_width,\n",
    "                 combination_image,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    \"\"\"\n",
    "    This is here because scipy.optimize requires seperate functions for loss and gradients,\n",
    "    which would be inefficient to compute.\n",
    "    This Evaluator allows use to compute loss and gradients in one pass.\n",
    "\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    :param fetch_loss_and_grads: Function for getting the current loss and gradient\n",
    "    :type  fetch_loss_and_grads: keras.backend.tensorflow_backend.Function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_height, img_width, fetch_loss_and_grads):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.fetch_loss_and_grads = fetch_loss_and_grads\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, self.img_height, self.img_width, 3))\n",
    "        outs = self.fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.backend.tensorflow_backend.Function"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_prefix = 'my_result'\n",
    "grads = K.gradients(loss, combination_image)[0]\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
    "evaluator = Evaluator(img_height, img_width, fetch_loss_and_grads = K.function([combination_image], [loss, grads]))\n",
    "type (fetch_loss_and_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_generate_img(\n",
    "        target_file,\n",
    "        style_file,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        evaluator,\n",
    "        iterations=20):\n",
    "    \"\"\"\n",
    "    :param target_file: Name of target image\n",
    "    :type  target_file: str\n",
    "    :param style_file: Name of style image\n",
    "    :type  style_file: str\n",
    "    :param img_height: height of image\n",
    "    :type  img_height: int\n",
    "    :param img_width: width of image\n",
    "    :type  img_width: int\n",
    "    :param evaluator: class for producing loss and gradients\n",
    "    :type  evaluator: class\n",
    "    :param iterations: Number of training iterations\n",
    "    :type  iterations: int\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.optimize import fmin_l_bfgs_b\n",
    "    from scipy.misc import imsave\n",
    "    import time\n",
    "\n",
    "    result_prefix = 'images/generated/' + \\\n",
    "        target_file.split('.')[0] + '_' + \\\n",
    "        style_file.split('.')[0]\n",
    "\n",
    "    target_image_path, style_reference_image_path = get_img_paths(\n",
    "        target_file, style_file)\n",
    "    x = preprocess_image(target_image_path, img_height, img_width)\n",
    "    # scipy.optimize.fmin_l_bfgs_b can only process flat vectors\n",
    "    x = x.flatten()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print('Start of iteration', i)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Runs L-BFGS optimization over the pixels of the generated image\n",
    "        # to minimize the neural style loss.\n",
    "        x, min_val, info = fmin_l_bfgs_b(\n",
    "            evaluator.loss,\n",
    "            x,\n",
    "            fprime=evaluator.grads,\n",
    "            maxfun=20\n",
    "        )\n",
    "        print('Current loss value:', min_val)\n",
    "        img = x.copy().reshape((img_height, img_width, 3))\n",
    "        img = deprocess_image(img)\n",
    "        fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "        imsave(fname, img)\n",
    "        print('Image saved as', fname)\n",
    "        end_time = time.time()\n",
    "        print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_and_generate_img(target_file, style_file, img_height, img_width, iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_image(\n",
    "        target_file,\n",
    "        style_file,\n",
    "        total_variation_weight=1e-4,\n",
    "        style_weight=1.,\n",
    "        content_weight=0.025,\n",
    "        iterations=20):\n",
    "    \"\"\"\n",
    "    Produces a style transfered image given a base image and a style image.\n",
    "\n",
    "    :param target_file: Name of target image\n",
    "    :type  target_file: str\n",
    "    :param style_file: Name of style image\n",
    "    :type  style_file: str\n",
    "    :param total_variation_weight: Weight of the total variation loss\n",
    "    :type  total_variation_weight: float\n",
    "    :param style_weight: Weight of the style loss\n",
    "    :type  style_weight: float\n",
    "    :param content_weight: Weight of the content loss\n",
    "    :type  content_weight: float\n",
    "    :param iterations: Number of training iterations\n",
    "    :type  iterations: int\n",
    "    \"\"\"\n",
    "\n",
    "    target_image_path, style_reference_image_path = get_img_paths(\n",
    "        target_file, style_file)\n",
    "    img_height, img_width = get_width_height(target_image_path)\n",
    "    model, combination_image = create_model(\n",
    "        target_image_path,\n",
    "        style_reference_image_path,\n",
    "        img_height, img_width\n",
    "    )\n",
    "    model.summary()\n",
    "    loss = find_loss(model, img_height, img_width, combination_image)\n",
    "\n",
    "    # get gradients of generated image with regard to the loss\n",
    "    grads = K.gradients(loss, combination_image)[0]\n",
    "    # gets the value of the current loss and current gradients\n",
    "    fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
    "\n",
    "    evaluator = Evaluator(img_height, img_width, fetch_loss_and_grads)\n",
    "    train_and_generate_img(\n",
    "        target_file,\n",
    "        style_file,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        evaluator,\n",
    "        iterations=iterations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "target_image_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "combination_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "content_loss <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix x <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "gram_matrix <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "style_reference_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "combination_features <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "style_loss <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "combination_image <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "total_variation_loss <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Start of iteration 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3df78f62b777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Empire-State-Building.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Natasha-Russu-Eye.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-8cb31efc31fb>\u001b[0m in \u001b[0;36mgenerate_image\u001b[0;34m(target_file, style_file, total_variation_weight, style_weight, content_weight, iterations)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-15-9f2fea348c16>\u001b[0m in \u001b[0;36mtrain_and_generate_img\u001b[0;34m(target_file, style_file, img_height, img_width, evaluator, iterations)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current loss value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 193\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    195\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9ff468512c5c>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_image('Empire-State-Building.jpg', 'Natasha-Russu-Eye.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
